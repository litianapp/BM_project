---
title: "Data Exploration"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(corrplot)
library(patchwork)
library(HH)
library(leaps)
library(modelr)

```

Load the cancer registry data

```{r message = FALSE, warning = FALSE}

cancer_data = read_csv(file = "./data/Cancer_Registry.csv") %>% 
  janitor::clean_names()
```

Obtain summary statistics for all variables

```{r}

numerical = cancer_data%>%
  select(-c(binned_inc, geography))

Minimum = sapply(numerical, min, na.rm = 'TRUE')
Maximum = sapply(numerical, max, na.rm = 'TRUE')
Mean = sapply(numerical, mean, na.rm = 'TRUE')
Median = sapply(numerical, median, na.rm = 'TRUE')
SD = sapply(numerical, sd, na.rm = 'TRUE')
IQR = sapply(numerical, IQR, na.rm='TRUE')
Size = sapply(numerical, length)
Missing = sapply(numerical, function(x) sum(is.na(x)))

rbind(Minimum, Maximum, Mean, SD, Median, IQR, Size, Missing) %>%
  round(digits = 2)

```

Note that: some_college = 2285 out of 3047 missing values, employed = 152 out of 3047 missing values, and private_coverage_only = 609 out of 3047 missing values. Some college has too many missing values so it might be advisable not include it in the list of potential covariates

Create a separate dataset without the missing values

```{r}

clean_cancer_data = cancer_data%>%
  select(-pct_some_col18_24)%>%
  na.omit()


```

Figure out if there are variables that are highly correlated/provide the same information

```{r}
continuous_data = clean_cancer_data%>%
  dplyr::select(-c(binned_inc, geography))

par(mfrow=c(1,1))
cor(continuous_data)%>%
  corrplot(method = "circle", diag=FALSE)
```

From literature review and preliminary discussions, we decided to remove binned_inc, pct_some_col_18_24, median_age, geography, avg_eaths_per_year, and pop_est_2015 from the saturated model.

Now, lets observe the distributions of potential covariates and observe if transformations are needed

```{r}

reduced_clean = clean_cancer_data%>%
  select(-c(binned_inc, geography, median_age, avg_deaths_per_year, pop_est2015))
```

Create a function that plots and performs tranformations on the variables

```{r warning = FALSE, message = FALSE}

distribution = function(variable) {

  reduced_clean = tibble(variable)
  
  reduced_clean%>%
    ggplot(aes(variable))+
    geom_histogram()
}

inspection = function(variable) {

  reduced_clean = tibble(variable)

  #Observe transformations

  transformations = reduced_clean%>%
    dplyr::select(variable)%>%
    mutate(log_variable = log(variable),
          sqrt_variable = sqrt(variable),
          inverse_variable = 1 / variable)%>%
    gather(key = type, value = value)
  
  a = transformations%>%
    filter(type == "variable")%>%
    ggplot(aes(value))+
    geom_density()+
    labs(x = "Original",
        y = "Frequency")

  b = transformations%>%
    filter(type == "sqrt_variable")%>%
    ggplot(aes(value))+
    geom_density()+
    labs(x = "Sqrt(Variable)",
        y = "Frequency")

  c = transformations%>%
    filter(type == "inverse_variable")%>%
    ggplot(aes(value))+
    geom_density()+
    labs(x = "Inverse(Variable)",
        y = "Frequency")

  d = transformations%>%
    filter(type == "log_variable")%>%
    ggplot(aes(value))+
    geom_density()+
    labs(x = "log(Variable)",
        y = "Frequency")

  (a + b) / (c + d)
  
}

```


Average annual count has one extreme outlier, making the distribution of the points to be right skewed. Log tranformation most reduces the skewness

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$avg_ann_count

distribution(variable)
inspection(variable)

#perform a log tranformation
model_data = reduced_clean%>%
  mutate(log_avg_ann_count = log(avg_ann_count))

```

The distribution of incidence rate seems to be appropriately clustered, with a few outliers. Transformations result in similar shapes as the original

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$incidence_rate

distribution(variable)
inspection(variable)
```

Median income is slighly right skewed and the inverse transformation reduces most of the skewness. Transformation also may not be necessary in this case

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$med_income

distribution(variable)
inspection(variable)

#perform an inverse tranformation
model_data = model_data%>%
  mutate(inverse_med_income = 1/(med_income))
```

Poverty percent is slightly right skewed and the square root transformation reduces most of the skewness. Transformation also may not be necessary in this case

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$poverty_percent

distribution(variable)
inspection(variable)

#perform a square root tranformation
model_data = model_data%>%
  mutate(sqrt_poverty_rate = sqrt(poverty_percent))

```

Study per cap is highly right skewed and the log transformation reduces most of the skewness

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$study_per_cap

distribution(variable)
inspection(variable)

#perform a log tranformation (add 0.01 since there are some 0 values)
model_data = model_data%>%
  mutate(study_per_cap = study_per_cap + 0.01,
         log_study_per_cap = log(study_per_cap))

```

Median age for males has an appropriate shape and does not need transformation

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$median_age_male

distribution(variable)
inspection(variable)


```

Median age for females has an appropriate shape and does not need transformation

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$median_age_female

distribution(variable)
inspection(variable)


```

Average household size has some unusual values. A majority of the points are clustured around 2.5 and there is a slight skewness to the right. However, there are some outliers to the left that are close to 0. None of the transformations are useful - the log transformation removes the slight right skewness but we still see a another bump to the left 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$avg_household_size

distribution(variable)
inspection(variable)

#Perform a log transformation
model_data = model_data%>%
  mutate(log_avg_household_size = log(avg_household_size))

```

Percent married is slightly left skewed but none of the transformation perform better, in terms of skewness

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$percent_married

distribution(variable)
inspection(variable)

```

pct_no_hs18_24 is slightly right skewed and the square root transformation reduces most of the skewness. However, a transformation might not be necessary in this case 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_no_hs18_24


distribution(variable)
inspection(variable)


#Perform a square root transformation
model_data = model_data%>%
  mutate(sqrt_pct_no_hs18_24 = sqrt(pct_no_hs18_24))

```

pct_hs18_24 is appropriately shaped 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_hs18_24


distribution(variable)
inspection(variable)

```

pct_bach_deg18_24 is right skewed. While the square root transformation reduces most of the skewness, it also creates a second peak close to zero. 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_bach_deg18_24


distribution(variable)
inspection(variable)

#Perform a square root transformation
model_data = model_data%>%
  mutate(sqrt_pct_bach_deg18_24 = sqrt(pct_bach_deg18_24))

```

pct_hs25_over is very slightly left skewed and no transformation is necessary 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_hs25_over


distribution(variable)
inspection(variable)

```

pct_bach_deg25_over is right skewed and a log transformation best reduces the skewness 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_bach_deg25_over


distribution(variable)
inspection(variable)


#Perform a log transformation
model_data = model_data%>%
  mutate(log_pct_bach_deg25_over = log(pct_bach_deg25_over))

```

pct_employed16_over is very slightly left skewed so no transformation is necessary

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_employed16_over


distribution(variable)
inspection(variable)

```

pct_unemployed16_over is right skewed and a square root transformation works best to reduce skewness

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_unemployed16_over


distribution(variable)
inspection(variable)

#Perform a square root transformation
model_data = model_data%>%
  mutate(sqrt_pct_unemployed16_over = sqrt(pct_unemployed16_over))


```

pct_private_coverage is appropriately shaped (very slight skewness) and no transformation is needed

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_private_coverage


distribution(variable)
inspection(variable)

```

pct_private_coverage_alone is appropriately shaped and no transformation is needed

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_private_coverage_alone


distribution(variable)
inspection(variable)

```

pct_emp_priv_coverage is appropriately shaped 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_emp_priv_coverage


distribution(variable)
inspection(variable)

```

pct_public_coverage is appropriately shaped 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_public_coverage


distribution(variable)
inspection(variable)

```

pct_public_coverage_alone is slightly skewed to the right and the square root transformation reduces the skewness. But a transformation may not be necessary in this case  

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_public_coverage_alone


distribution(variable)
inspection(variable)

#Perform a square root transformation
model_data = model_data%>%
  mutate(sqrt_pct_public_coverage_alone = sqrt(pct_public_coverage_alone))

```

pct_white is left skewed and no transformation improves the distribution

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_white


distribution(variable)
inspection(variable)

```

pct_black is right skewed. Although, log transformation best reduces the skewness, it creates an unusual shape where a majority of the points are concentrated towards the upper half of the curve

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_black + 0.01


distribution(variable)
inspection(variable)

#Perform a log transformation (add 0.01 since there are some 0 values)
model_data = model_data%>%
  mutate(pct_black = pct_black + 0.01,
         log_pct_black = log(pct_black))

```

pct_asian is right skewed and a log transformation reduces the skewness

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_asian + 0.01


distribution(variable)
inspection(variable)

#Perform a log transformation (add 0.01 since there are some 0 values)
model_data = model_data%>%
  mutate(pct_asian = pct_asian + 0.01,
         log_pct_asian = log(pct_asian))

```

pct_other_race is right skewed and a log transformation reduces the skewness

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_other_race + 0.01


distribution(variable)
inspection(variable)

#Perform a log transformation
model_data = model_data%>%
  mutate(pct_other_race = pct_other_race + 0.01,
         log_other_race = log(pct_other_race))

```

pct_married_households is appropriately shaped 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$pct_married_households


distribution(variable)
inspection(variable)

```

birth_rate is slightly right skewed and square root transformation reduces the skewness. Although, the transformation might be unnecessary 

```{r warning = FALSE, message = FALSE}

variable = reduced_clean$birth_rate


distribution(variable)
inspection(variable)

#Perform a square root transformation
model_data = model_data%>%
  mutate(sqrt_birth_rate = sqrt(birth_rate))

```


Create a full model (with all transformations, including questionable ones)

```{r eval = FALSE}

#check for NA 
model_data = model_data%>%
  na.omit()

#fit model

transform_model = lm(target_death_rate ~ log_avg_ann_count + incidence_rate + inverse_med_income + sqrt_poverty_rate + 
     median_age_male + median_age_female + log_avg_household_size + percent_married +
     sqrt_pct_no_hs18_24 + pct_hs18_24 + sqrt_pct_bach_deg18_24 + pct_hs25_over + pct_bach_deg25_over +
     pct_employed16_over + sqrt_pct_unemployed16_over + pct_private_coverage + pct_private_coverage_alone +
     pct_emp_priv_coverage + pct_public_coverage + sqrt_pct_public_coverage_alone + pct_white +
     pct_married_households + sqrt_birth_rate + log_study_per_cap + log_pct_black + log_pct_asian + log_other_race,
   data = model_data)


summary(transform_model)
```

Create a full model (with transformations, not including questionable ones)

```{r eval=FALSE}

non_transform_model = lm(target_death_rate ~ log_avg_ann_count + incidence_rate + med_income + poverty_percent +
     median_age_male + median_age_female + avg_household_size + percent_married +
     pct_no_hs18_24 + pct_hs18_24 + sqrt_pct_bach_deg18_24 + pct_hs25_over + pct_bach_deg25_over +
     pct_employed16_over + sqrt_pct_unemployed16_over + pct_private_coverage + 
     pct_private_coverage_alone + pct_emp_priv_coverage + pct_public_coverage + pct_public_coverage_alone +
     pct_white + pct_married_households + birth_rate + log_study_per_cap + log_pct_black + log_pct_asian + log_other_race,
   data = model_data)

summary(non_transform_model)
```

Before proceeding with the model selection process, it might be useful to determine what variables are highly correlated with each other in order decrease the number of variables in the full model

```{r}

vif(transform_model)
```

We find that the following variables have a VIF > 5: inverse_med_income, sqrt_poverty_rate, median_age_male, median_age_female, percent_married, pct_bach_deg25_over, pct_employed16_over, pct_private_coverage, pct_private_coverage_alone, pct_emp_priv_coverage, pct_public_coverage, sqrt_pct_public_coverage_alone, pct_married_households


```{r}

correlation_check = model_data %>%
  dplyr::select(inverse_med_income, sqrt_poverty_rate, median_age_male, median_age_female, percent_married, pct_bach_deg25_over, pct_employed16_over, pct_private_coverage, pct_private_coverage_alone, pct_emp_priv_coverage, pct_public_coverage, sqrt_pct_public_coverage_alone, pct_married_households)

cor(correlation_check)%>%
  corrplot(method = "square", addCoef.col = "black", tl.col="black", tl.srt=45, insig = "blank", 
         diag=FALSE, number.cex = .7)

```

We find that inverse_med_income is highly related to sqrt_poverty_rate, pct_employed16_over, pct_private_coverage, pct_private_coverage_alone, pct_emp_priv_coverage, pct_public_coverage, and sqrt_pct_coverage_alone. All variables concerning insurance are highly correlated to each other. Median_age_male is highly correlated to median_age_female and percent_married is highly correlated to percent_married_households. pct_bach_deg25_over is moderately correlated to the all the other variables, which is expected because it only had a VIF = 5.2796. Out of the 13 variables with high VIF, we can keep inverse_med_income (in place of sqrt_poverty_rate, pct_employed16_over, and all the insurance variables), pct_bach_deg25_over, percent_married (its correlation to inverse_med_income is lower than the correlation between pct_married_household and inverse_med_income), and median_age_female (its correlation to percent_married is lower than median_age_males's)

```{r}

reduced_transform_model = lm(target_death_rate ~ log_avg_ann_count + incidence_rate + inverse_med_income + 
                               median_age_female + log_avg_household_size + percent_married + sqrt_pct_no_hs18_24 + 
                               pct_hs18_24 + sqrt_pct_bach_deg18_24 + pct_hs25_over + pct_bach_deg25_over +
                               sqrt_pct_unemployed16_over + pct_white + sqrt_birth_rate + log_study_per_cap + 
                               log_pct_black + log_pct_asian + log_other_race, data = model_data)


vif(reduced_transform_model)

```

Now, collinearity has been reduced

#Model Selection

Begin the model selection process

1) Stepwise regression 

```{r}

step(reduced_transform_model, direction = "both")

```

2) Criterion based procedures

```{r}

best <- function(model, ...) 
{
  subsets <- regsubsets(formula(model), model.frame(model), nvmax=11, ...)
  subsets <- with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
}  


# Select the 'best' model of all subsets for 21-predictor model
round(best(reduced_transform_model, nbest = 1), 3)

```

Based on adjusted R-squared, Cp, and BIC, we arrive at a 10 predictor model which includes: incidence_rate, inverse_med_income, median_age_female, pct_hs18_24, pct_hs25_over, pct_bach_deg25_over, sqrt_pct_unemployed16_over, sqrt_birth_rate, log_pct_black, log_other_race. With these variables, Cp and BIC are minimized while adjusted R squared is maximized with the lowest possible number of variables.


#Model Diagnostics

Now, we will evaluate whether there is collinearity between our variables in the selected model

```{r}

selected_model = lm(target_death_rate ~ incidence_rate + inverse_med_income + median_age_female + 
                      pct_hs18_24 + pct_hs25_over + pct_bach_deg25_over + sqrt_pct_unemployed16_over + 
                      sqrt_birth_rate + log_pct_black + log_other_race, data = model_data) 

vif(selected_model)
```

There is no collinearity between our selected variables

Check whether assumptions for regression are violated

```{r}

par(mfrow=c(2,2))
plot(selected_model)
```


Check for outliers, leverage, and influential points

```{r}

#find outlier in Y
stu_res = rstandard(selected_model)
outliers_y = stu_res[abs(stu_res)>2.5]

length(outliers_y)

influence.measures(selected_model)%>%View()
```

There are 60 outliers in the observed values. From the diagnostic plots, we saw that points 920, 1034, and 228 appeared to be problematic. These three points are indeed outliers and appear to be extreme outliers. 

```{r}

#find influential points

distance = cooks.distance(selected_model)
distance[distance > 0.5]

```

There are no points that would be considered influential. All points have a cook's distance less than 0.5

#Model Validation

Perform a cross validation

```{r}

cv_df =
  crossv_mc(model_data, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(lin_mod = map(train, ~lm(target_death_rate ~ incidence_rate + inverse_med_income + median_age_female + 
                      pct_hs18_24 + pct_hs25_over + pct_bach_deg25_over + sqrt_pct_unemployed16_over + 
                      sqrt_birth_rate + log_pct_black + log_other_race, data = .x)),
         rmse_test_lin_mod = map2_dbl(lin_mod, test, ~rmse(model = .x, data = .y)),
         rmse_train_lin_mod = map2_dbl(lin_mod, train, ~rmse(model = .x, data = .y)))

cv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

mean(cv_df$rmse_train_lin_mod)
mean(cv_df$rmse_test_lin_mod)

```
